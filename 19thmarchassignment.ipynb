{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the solution of 19th march assignment\n"
     ]
    }
   ],
   "source": [
    "print(\"this is the solution of 19th march assignment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max scaling is a data normalization technique used in data preprocessing to rescale numeric features to a fixed range (usually between 0 and 1). It is achieved by subtracting the minimum value and dividing by the range of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.291579</td>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.152283</td>\n",
       "      <td>0.073333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375786</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.431713</td>\n",
       "      <td>0.256667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450775</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.543779</td>\n",
       "      <td>0.546667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.505027</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.410557</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.308965</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.329074</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill       tip\n",
       "0      0.291579  0.001111\n",
       "1      0.152283  0.073333\n",
       "2      0.375786  0.277778\n",
       "3      0.431713  0.256667\n",
       "4      0.450775  0.290000\n",
       "..          ...       ...\n",
       "239    0.543779  0.546667\n",
       "240    0.505027  0.111111\n",
       "241    0.410557  0.111111\n",
       "242    0.308965  0.083333\n",
       "243    0.329074  0.222222\n",
       "\n",
       "[244 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "tips = sns.load_dataset('tips')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(tips[['total_bill','tip']])\n",
    "pd.DataFrame(scaler.transform(tips[['total_bill','tip']]),columns=['total_bill','tip'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit Vector technique scales features by dividing each feature value by its magnitude to obtain a unit vector. Unlike Min-Max scaling, it ensures that all features have the same scale and preserves the direction of the original data, but it may not be appropriate for sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106932</td>\n",
       "      <td>0.467827</td>\n",
       "      <td>0.143690</td>\n",
       "      <td>0.865480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074610</td>\n",
       "      <td>0.472215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.084285</td>\n",
       "      <td>0.461417</td>\n",
       "      <td>0.145192</td>\n",
       "      <td>0.871154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.164486</td>\n",
       "      <td>0.576768</td>\n",
       "      <td>0.131375</td>\n",
       "      <td>0.789318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.132330</td>\n",
       "      <td>0.551374</td>\n",
       "      <td>0.067390</td>\n",
       "      <td>0.820935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>0.094955</td>\n",
       "      <td>0.569731</td>\n",
       "      <td>0.134203</td>\n",
       "      <td>0.805220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>0.221274</td>\n",
       "      <td>0.684840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>0.173037</td>\n",
       "      <td>0.668743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.723078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>0.122571</td>\n",
       "      <td>0.656633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>0.150141</td>\n",
       "      <td>0.584966</td>\n",
       "      <td>0.131032</td>\n",
       "      <td>0.786195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6433 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      distance      fare       tip     total\n",
       "0     0.106932  0.467827  0.143690  0.865480\n",
       "1     0.074610  0.472215  0.000000  0.878320\n",
       "2     0.084285  0.461417  0.145192  0.871154\n",
       "3     0.164486  0.576768  0.131375  0.789318\n",
       "4     0.132330  0.551374  0.067390  0.820935\n",
       "...        ...       ...       ...       ...\n",
       "6428  0.094955  0.569731  0.134203  0.805220\n",
       "6429  0.221274  0.684840  0.000000  0.694286\n",
       "6430  0.173037  0.668743  0.000000  0.723078\n",
       "6431  0.122571  0.656633  0.000000  0.744184\n",
       "6432  0.150141  0.584966  0.131032  0.786195\n",
       "\n",
       "[6433 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxis = sns.load_dataset('taxis')\n",
    "from sklearn.preprocessing import normalize\n",
    "pd.DataFrame(normalize(taxis[['distance','fare','tip','total']]),columns=['distance','fare','tip','total'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PCA is a statistical technique used for dimensionality reduction. It involves finding the most significant features or components of a dataset and representing it in a lower-dimensional space. For example, if we have a dataset with many features, PCA can identify which features are most important and create a new dataset with fewer features that still capture most of the information in the original data. An example of PCA's application is reducing the dimensions of images to improve computer vision algorithms' performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PCA and feature extraction are related concepts, with PCA being one way to perform feature extraction. Feature extraction is the process of identifying and selecting relevant features or attributes from a dataset, with the goal of reducing its dimensionality or improving its quality for further analysis or modeling. PCA can be used for feature extraction by identifying the most significant components of the dataset and selecting a subset of those components as features. This can result in a smaller and more meaningful set of features that can improve the performance of machine learning algorithms.\n",
    "\n",
    "For example, suppose we have a dataset of images for object recognition. Each image contains many pixels, and each pixel is a feature. Using PCA, we can identify the most important components, which represent the major patterns in the images, and use these components as features for object recognition, resulting in a more efficient and accurate model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Min-Max scaling is a common preprocessing technique used in machine learning to transform numerical features to a common scale. This technique is used to normalize the data to a common range, typically between 0 and 1, by subtracting the minimum value and dividing by the range.\n",
    "\n",
    "In the case of a recommendation system for a food delivery service, we could use Min-Max scaling to preprocess features such as price, rating, and delivery time. \n",
    "\n",
    "For example, let's say we have a feature 'price' with values ranging from $5 to $25. We can use the Min-Max scaling technique to transform these values to a range between 0 and 1. To do this, we would first subtract the minimum value of $5 from all the values, and then divide by the range of $20 (i.e., max value of $25 minus the min value of $5).\n",
    "\n",
    "Thus, the new range of the 'price' feature would be between 0 and 1. A value of $10 would be transformed to (10-5)/20 = 0.25, and a value of $20 would be transformed to (20-5)/20 = 0.75.\n",
    "\n",
    "Similarly, we could use Min-Max scaling to preprocess other numerical features such as rating and delivery time. By scaling all the features to the same range, we can ensure that they are equally weighted in the recommendation system, and that no feature dominates the others due to differences in their magnitude. \n",
    "\n",
    "Overall, Min-Max scaling is a useful technique for preprocessing numerical features and can improve the performance of recommendation systems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PCA (Principal Component Analysis) is a technique used to reduce the dimensionality of the dataset by transforming the original features into a new set of linearly uncorrelated variables called principal components. In the context of a stock price prediction project, PCA can be used to reduce the number of features by extracting the most significant features that explain the variance in the data.\n",
    "\n",
    "The steps to use PCA to reduce the dimensionality of the dataset are as follows:\n",
    "\n",
    "1. Standardize the data: Before applying PCA, it is important to standardize the data to ensure that each feature has a similar scale. This is done by subtracting the mean of each feature and dividing by its standard deviation.\n",
    "\n",
    "2. Compute the covariance matrix: The next step is to compute the covariance matrix of the standardized data. The covariance matrix is a matrix that shows how much two variables are related to each other. It contains the variances of each feature on the diagonal and the covariances between each pair of features on the off-diagonal.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues of the covariance matrix are computed next. The eigenvectors are the directions in which the data varies the most, and the corresponding eigenvalues indicate how much variance is explained by each eigenvector.\n",
    "\n",
    "4. Choose the number of principal components: The number of principal components to keep is determined by looking at the eigenvalues and selecting the top k eigenvectors that explain the majority of the variance in the data. A common rule of thumb is to keep enough principal components to explain 80-90% of the variance in the data.\n",
    "\n",
    "5. Project the data onto the new feature space: The final step is to project the original data onto the new feature space defined by the top k eigenvectors. This produces a new dataset with a reduced number of features that can be used for further analysis or modeling.\n",
    "\n",
    "By using PCA to reduce the dimensionality of the dataset, we can simplify the model and improve its performance by eliminating redundant and noisy features. Additionally, PCA can help identify the most important variables that contribute the most to the prediction of the stock prices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,5,10,15,20]\n",
    "x = pd.DataFrame(data)\n",
    "scaler.fit(x)\n",
    "scaler.transform(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PCA (Principal Component Analysis) is a popular technique used for feature extraction in data analysis. It is used to reduce the number of dimensions in a dataset by finding a smaller number of latent variables or principal components that explain the maximum amount of variance in the original data. \n",
    "\n",
    "In this case, we have a dataset with five features: height, weight, age, gender, and blood pressure. To perform feature extraction using PCA, we would first need to standardize the data by subtracting the mean and dividing by the standard deviation for each feature. This ensures that each feature has equal weight in the analysis.\n",
    "\n",
    "Once the data is standardized, we can perform PCA to extract the principal components. The number of principal components to retain would depend on the amount of variance we want to explain in the data. We can use the scree plot to visualize the amount of variance explained by each principal component and choose a cutoff based on a threshold.\n",
    "\n",
    "For example, if we want to retain at least 95% of the variance in the data, we would choose the number of principal components that explain at least 95% of the total variance. The exact number of principal components would depend on the data and can be determined by inspecting the scree plot.\n",
    "\n",
    "In general, it is recommended to retain at least 80% to 90% of the variance in the data to ensure that important information is not lost during dimensionality reduction. Therefore, in this case, I would choose to retain enough principal components that explain at least 80% to 90% of the total variance in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
